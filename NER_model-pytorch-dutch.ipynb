{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4975e59e-2fc5-4452-8dd9-00c729c0b8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niels\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "import random\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Ensure you have the necessary NLTK tokenizer models downloaded\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58cbc494-eedd-4d55-84a8-4bebe30c2932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Niels\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers.keras_callbacks import KerasMetricCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee86521-2c23-4f75-827c-05928b31bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# folder_path = '/content/drive/MyDrive/UU/Thesis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea05ca1-440d-4517-aca1-1e6c49660574",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef3cfbf-f761-48a8-81db-dbdd2688701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an Excel file into a DataFrame\n",
    "df = pd.read_excel('LCReviewsIntegrated_1962-1994.xlsx', engine='openpyxl')\n",
    "# df = pd.read_excel(folder_path + '/LCReviewsIntegrated_1962-1994.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb5aa31c-73ca-4e26-aab5-a3b7e144714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(text):\n",
    "    # Replace multiple spaces with a single space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text)\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6109b689-85d5-40d1-8ddd-2fb0c0311750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(remove_extra_spaces)\n",
    "df['title1'] = df['title1'].apply(remove_extra_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4157b17-9ea9-47f7-a996-4b3574a5966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(input_string):\n",
    "    # Create a translation table that maps each punctuation character to None\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # Translate the input string using the translation table\n",
    "    return input_string.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7cb13c9-f6ec-4e44-8711-5799d5fe5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(x):\n",
    "    if not (type(x.title1) == str):\n",
    "        return np.nan\n",
    "    if not (type(x.content) == str):\n",
    "        return np.nan\n",
    "    if x.title1.lower() in x.content.lower():\n",
    "        return x.title1\n",
    "    \n",
    "    sentence_parts = re.split(r' / | : ', x.title1.lower())\n",
    "    sentence_parts = sorted(sentence_parts, key=len, reverse=True)\n",
    "    for part in sentence_parts:\n",
    "        if part in x.content.lower():\n",
    "            return part\n",
    "        elif remove_punctuation(part) in x.content.lower():\n",
    "            return remove_punctuation(part)\n",
    "\n",
    "    if remove_punctuation(x.title1).lower() in x.content.lower():\n",
    "        return remove_punctuation(x.title1)\n",
    "        \n",
    "    return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecc828c7-ca29-4c6c-9b9e-393b6ad465dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentence_in_text(full_text, sentence):\n",
    "    start_index = full_text.find(sentence)\n",
    "    if start_index == -1:\n",
    "        print(\"EROR!!!\")\n",
    "        return False\n",
    "    end_index = start_index + len(sentence)\n",
    "    return start_index, end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "140311f3-6370-452b-a74c-37959ac1f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for index, row in df.iterrows():\n",
    "    result.append(extract_title(row))\n",
    "\n",
    "df['title3'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fb92165-3353-440b-8918-2bebd9dff278",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df[df['title3'] != 'error']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35afb79-0f32-42ba-933a-ed3cfb872484",
   "metadata": {},
   "source": [
    "## Check what tokens are present before the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32a36f9d-32c0-4082-b469-343b589e2d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_for_sentence(full_text, sentence):\n",
    "    # Tokenize the full text\n",
    "    tokens = word_tokenize(full_text)\n",
    "    # Find the start and end indices of the sentence in the full text\n",
    "    start_index = full_text.find(sentence)\n",
    "    if start_index == -1:\n",
    "        return None, None\n",
    "    end_index = start_index + len(sentence)\n",
    "    # Tokenize the sentence separately to match tokens exactly\n",
    "    sentence_tokens = word_tokenize(sentence)\n",
    "    # Initialize mask with zeros\n",
    "    mask = [0] * len(tokens)\n",
    "    # Loop through the full text tokens to set the mask\n",
    "    sentence_pos = 0\n",
    "    for i, token in enumerate(tokens):\n",
    "        if sentence_pos < len(sentence_tokens) and token == sentence_tokens[sentence_pos]:\n",
    "            # Check if the full sequence matches and ensure it doesn't go out of bounds\n",
    "            if i + len(sentence_tokens) - sentence_pos <= len(tokens) and \\\n",
    "                all(tokens[i + j] == sentence_tokens[sentence_pos + j] for j in range(len(sentence_tokens) - sentence_pos)):\n",
    "                # Mark the mask for the length of the sentence tokens\n",
    "                mask[i:i + len(sentence_tokens)] = [1] * len(sentence_tokens)\n",
    "                break\n",
    "            else:\n",
    "                # Increment if it's not a full sequence match\n",
    "                sentence_pos += 1\n",
    "        elif token == sentence_tokens[0]:  # Reset if it's the beginning of sentence tokens\n",
    "            sentence_pos = 0\n",
    "    return tokens, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fec478bd-e1c4-45cb-afdb-2af96f4f3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\"  # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "# model_checkpoint = \"distilbert-base-uncased\"\n",
    "model_checkpoint = \"Babelscape/wikineural-multilingual-ner\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93c61150-d612-4060-875f-d2466de108cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f53e4847-c14e-43ac-ae7d-d1d6352b64e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_set(samples):\n",
    "    data = []\n",
    "\n",
    "    for sample in samples:\n",
    "        unique_content_df = df_clean[df_clean['content'] == sample]\n",
    "        masks = []\n",
    "    \n",
    "        review = remove_punctuation(sample.lower())\n",
    "    \n",
    "        for index, row in unique_content_df.iterrows():\n",
    "            book = remove_punctuation(row['title3'].lower())\n",
    "            \n",
    "            tokens, mask = create_mask_for_sentence(review, book)  # Assuming this returns a mask of the same length as tokens\n",
    "            masks.append(mask)\n",
    "        masks = np.bitwise_or.reduce(np.array(masks), axis=0)\n",
    "        mask[mask == 0] = \"NO_BOOK\"\n",
    "        mask[mask == 1] = \"BOOK\"\n",
    "        \n",
    "        data.append({\n",
    "            \"tokens\": tokens,\n",
    "            \"ner_tags\": masks\n",
    "        })\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52c87fb3-4156-4edf-b034-3aaa9f51b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.random.choice(df_clean['content'].unique(), size=100, replace=False)\n",
    "\n",
    "train_dataset = Dataset.from_list(create_data_set(samples[:int(len(samples) * 0.8)]))\n",
    "val_dataset = Dataset.from_list(create_data_set(samples[int(len(samples) * 0.8):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9ebcb9a-38ce-406a-ab9d-1205fec0979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "482ebb52-8591-44c1-ade3-7d7cde9b57e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [00:00<00:00, 256.23 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 301.25 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 80\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset_train = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_dataset_val = val_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "tokenized_dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62b103e7-94c4-4ad2-88c0-e0148dc72be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niels\\AppData\\Local\\Temp\\ipykernel_30748\\801905788.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n",
      "C:\\Users\\Niels\\miniconda3\\Lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [l for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a254f89-f828-488e-8d24-e6d4f15ef252",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4eca33a-b0f4-451e-8a0e-b4ba61612040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at Babelscape/wikineural-multilingual-ner and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=2, ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18b3a2d7-b9b1-4e92-b97d-1ba57ae4e09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niels\\miniconda3\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_train,\n",
    "    eval_dataset=tokenized_dataset_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5b0ba9d-fc0b-4161-a440-39bf13f7a759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 05:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.087713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.979167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niels\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: 0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\Niels\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: 1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\Niels\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Niels\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Niels\\miniconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "C:\\Users\\Niels\\miniconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40, training_loss=0.08235253691673279, metrics={'train_runtime': 355.5916, 'train_samples_per_second': 0.225, 'train_steps_per_second': 0.112, 'total_flos': 20545478188824.0, 'train_loss': 0.08235253691673279, 'epoch': 1.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad66b3a-a7c7-487e-a7df-5f4aabb44ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ab1052-c9ee-41b9-a5c6-5cd58388c77a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42541d7-7a9f-4627-9db7-5035e8493a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211c96c7-a1a2-4a69-8f9b-3eeae799fad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c666e29-37cc-4995-82cd-bf3917f611c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28deab19-d28e-4dda-b503-dc3a4a1aee8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf60a3b6-6dd3-45cf-b5f2-2ccde47af4c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m tokenizer([\u001b[43msample\u001b[49m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer([sample], return_tensors=\"np\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd6205-1af8-482e-9801-e11933687d33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd02b5b-a6d8-4ef5-9ac1-368f33faca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(tokenized).logits\n",
    "classes = np.argmax(outputs, axis=-1)[0]\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84442245-a3f1-4a29-bb3c-332f050e9081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e31152-3789-4527-9ffa-00c520a53e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f904b1-b79d-4676-903c-c7926f27c1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
